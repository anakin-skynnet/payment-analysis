# =============================================================================
# Step 2: Simulate payment transaction events (producer)
# =============================================================================
# Generates synthetic payment events at high volume for real-time pipeline testing.
# Events are ingested later by Lakeflow pipelines (step 3 / continuous ingestion).
# Catalog/schema from variables.
# =============================================================================

resources:
  jobs:
    "job_2_simulate_transaction_events":
      name: "[${var.environment}] 2. Simulate Transaction Events (Producer)"
      description: "Generate synthetic payment events at high volume for real-time pipeline testing. Events will be ingested into the solution by Lakeflow and step 3."
      tasks:
        - task_key: generate_transactions
          description: "Generate synthetic payment events at high volume"
          notebook_task:
            notebook_path: ${workspace.file_path}/src/payment_analysis/streaming/transaction_simulator
            base_parameters:
              catalog: ${var.catalog}
              schema: ${var.schema}
              events_per_second: "1000"
              duration_minutes: "60"
              output_mode: "delta"
          new_cluster:
            spark_version: "15.4.x-scala2.12"
            node_type_id: "Standard_DS3_v2"
            autoscale:
              min_workers: 1
              max_workers: 2
            spark_conf:
              spark.databricks.delta.optimizeWrite.enabled: "true"
              spark.databricks.delta.autoCompact.enabled: "true"
            spark_env_vars:
              EVENTS_PER_SECOND: "1000"
          timeout_seconds: 3600
      tags:
        environment: ${var.environment}
        project: payment-analysis
        component: simulator
        step: "2"
        type: streaming
        domain: payment-analytics
