# =============================================================================
# Lakeflow — Pipelines 7 & 8 (run after jobs 1–6 when needed)
# =============================================================================
# Two pipelines: main ETL (Bronze → Silver → Gold) and dedicated real-time stream.
# Numeric prefix matches execution order (jobs 1–6, then pipelines 7, 8).
# Catalog/schema from variables. Serverless, continuous, Photon. Development mode in dev.
# =============================================================================

resources:
  pipelines:
    # 7. Main ETL: bronze_ingest → silver_transform → gold_views (notebooks)
    payment_analysis_etl:
      name: "[${var.environment}] 7. Payment Analysis ETL"
      catalog: ${var.catalog}
      schema: ${var.schema}
      channel: CURRENT
      configuration:
        catalog_name: ${var.catalog}
        schema_name: ${var.schema}
        # Delta and schema evolution
        spark.databricks.delta.preview.enabled: "true"
        spark.databricks.delta.schema.autoMerge.enabled: "true"
        pipelines.tableMaterialization.autoPropagate: "true"
        # Write and compaction
        spark.databricks.delta.optimizeWrite.enabled: "true"
        spark.databricks.delta.autoCompact.enabled: "true"
        # Streaming checkpoint
        spark.databricks.streaming.statefulOperator.asyncCheckpoint.enabled: "true"
        pipelines.trigger.interval: "2 seconds"
        pipelines.enableAutoOptimize: "true"
      libraries:
        - notebook:
            path: ${workspace.file_path}/src/payment_analysis/streaming/bronze_ingest
        - notebook:
            path: ${workspace.file_path}/src/payment_analysis/transform/silver_transform
        - notebook:
            path: ${workspace.file_path}/src/payment_analysis/transform/gold_views
      serverless: true
      continuous: true
      development: true
      photon: true

    # 8. Dedicated real-time pipeline (single notebook)
    payment_realtime_pipeline:
      name: "[${var.environment}] 8. Payment Real-Time Stream"
      catalog: ${var.catalog}
      schema: ${var.schema}
      channel: CURRENT
      configuration:
        catalog_name: ${var.catalog}
        schema_name: ${var.schema}
        spark.databricks.delta.optimizeWrite.enabled: "true"
        spark.databricks.delta.autoCompact.enabled: "true"
        spark.databricks.streaming.statefulOperator.asyncCheckpoint.enabled: "true"
        pipelines.trigger.interval: "2 seconds"
        pipelines.enableAutoOptimize: "true"
      libraries:
        - notebook:
            path: ${workspace.file_path}/src/payment_analysis/streaming/realtime_pipeline
      serverless: true
      continuous: true
      development: true
      photon: true
