# =============================================================================
# Step 6: Create and deploy Databricks AgentBricks agents
# =============================================================================
# One job with seven tasks: orchestrator plus six specialist agents (Smart Routing,
# Smart Retry, Decline Analyst, Risk Assessor, Performance Recommender) and a
# test run of the agent framework. Each task runs the same agent_framework notebook
# with a different agent_role. Catalog/schema from variables.
# Mosaic AI Gateway: configured on model serving and LLM endpoints in Serving UI.
# =============================================================================

resources:
  jobs:
    "job_6_deploy_agents":
      name: "[${var.environment}] 6. Deploy AgentBricks Agents (Orchestrator & Specialists)"
      description: "Create and deploy AgentBricks agents: orchestrator and specialists (Smart Routing, Smart Retry, Decline Analyst, Risk Assessor, Performance Recommender). Includes agent framework test."
      max_concurrent_runs: 1
      tasks:
        - task_key: run_orchestrator
          description: "Run payment analysis orchestrator for all agents"
          notebook_task:
            notebook_path: ${workspace.file_path}/src/payment_analysis/agents/agent_framework
            base_parameters:
              catalog: ${var.catalog}
              schema: ${var.schema}
              agent_role: "orchestrator"
              query: "Run comprehensive payment analysis (routing, retries, declines, risk, performance)"
          new_cluster:
            spark_version: "15.4.x-scala2.12"
            node_type_id: "Standard_DS3_v2"
            num_workers: 0
            spark_conf:
              spark.databricks.cluster.profile: singleNode
              spark.master: "local[*]"
            custom_tags:
              ResourceClass: SingleNode
          timeout_seconds: 3600

        - task_key: run_smart_routing
          description: "Run Smart Routing agent"
          notebook_task:
            notebook_path: ${workspace.file_path}/src/payment_analysis/agents/agent_framework
            base_parameters:
              catalog: ${var.catalog}
              schema: ${var.schema}
              agent_role: "smart_routing"
              query: "Analyze routing performance and recommend cascade configurations"
          new_cluster:
            spark_version: "15.4.x-scala2.12"
            node_type_id: "Standard_DS3_v2"
            num_workers: 0
            spark_conf:
              spark.databricks.cluster.profile: singleNode
              spark.master: "local[*]"
            custom_tags:
              ResourceClass: SingleNode
          timeout_seconds: 1800

        - task_key: run_smart_retry
          description: "Run Smart Retry agent"
          notebook_task:
            notebook_path: ${workspace.file_path}/src/payment_analysis/agents/agent_framework
            base_parameters:
              catalog: ${var.catalog}
              schema: ${var.schema}
              agent_role: "smart_retry"
              query: "Identify retry opportunities and recovery strategies"
          new_cluster:
            spark_version: "15.4.x-scala2.12"
            node_type_id: "Standard_DS3_v2"
            num_workers: 0
            spark_conf:
              spark.databricks.cluster.profile: singleNode
              spark.master: "local[*]"
            custom_tags:
              ResourceClass: SingleNode
          timeout_seconds: 1800

        - task_key: run_decline_analyst
          description: "Run Decline Analyst agent"
          notebook_task:
            notebook_path: ${workspace.file_path}/src/payment_analysis/agents/agent_framework
            base_parameters:
              catalog: ${var.catalog}
              schema: ${var.schema}
              agent_role: "decline_analyst"
              query: "Analyze decline patterns and recovery recommendations"
          new_cluster:
            spark_version: "15.4.x-scala2.12"
            node_type_id: "Standard_DS3_v2"
            num_workers: 0
            spark_conf:
              spark.databricks.cluster.profile: singleNode
              spark.master: "local[*]"
            custom_tags:
              ResourceClass: SingleNode
          timeout_seconds: 1800

        - task_key: run_risk_assessor
          description: "Run Risk Assessor agent"
          notebook_task:
            notebook_path: ${workspace.file_path}/src/payment_analysis/agents/agent_framework
            base_parameters:
              catalog: ${var.catalog}
              schema: ${var.schema}
              agent_role: "risk_assessor"
              query: "Identify high-risk transactions and interventions"
          new_cluster:
            spark_version: "15.4.x-scala2.12"
            node_type_id: "Standard_DS3_v2"
            num_workers: 0
            spark_conf:
              spark.databricks.cluster.profile: singleNode
              spark.master: "local[*]"
            custom_tags:
              ResourceClass: SingleNode
          timeout_seconds: 1800

        - task_key: run_performance_recommender
          description: "Run Performance Recommender agent"
          notebook_task:
            notebook_path: ${workspace.file_path}/src/payment_analysis/agents/agent_framework
            base_parameters:
              catalog: ${var.catalog}
              schema: ${var.schema}
              agent_role: "performance_recommender"
              query: "Analyze performance and recommend optimizations"
          new_cluster:
            spark_version: "15.4.x-scala2.12"
            node_type_id: "Standard_DS3_v2"
            num_workers: 0
            spark_conf:
              spark.databricks.cluster.profile: singleNode
              spark.master: "local[*]"
            custom_tags:
              ResourceClass: SingleNode
          timeout_seconds: 1800

        - task_key: test_agent_framework
          description: "Test multi-agent framework (orchestrator and specialists)"
          notebook_task:
            notebook_path: ${workspace.file_path}/src/payment_analysis/agents/agent_framework
            base_parameters:
              catalog: ${var.catalog}
              schema: ${var.schema}
              test_mode: "true"
          new_cluster:
            spark_version: "15.4.x-cpu-ml-scala2.12"
            node_type_id: "Standard_DS3_v2"
            num_workers: 0
            spark_conf:
              spark.databricks.cluster.profile: singleNode
              spark.master: "local[*]"
            spark_env_vars:
              MLFLOW_TRACKING_URI: "databricks"
            custom_tags:
              ResourceClass: SingleNode
          timeout_seconds: 1800
      schedule:
        quartz_cron_expression: "0 0 7 ? * MON"
        timezone_id: "UTC"
        pause_status: "PAUSED"
      tags:
        environment: ${var.environment}
        project: payment-analysis
        component: ai-agents
        step: "6"
        domain: payment-analytics
